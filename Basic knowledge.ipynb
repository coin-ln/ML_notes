{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计学习的分类"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本分类\n",
    "<font face='黑体' size=2>\n",
    "监督学习\n",
    "    :指从\" 标注数据 \"中学习预测模型的机器学习问题.<p></p>\n",
    "&emsp;&emsp;实质:学习输入到输出的映射的统计规律<p></p>\n",
    "无监督学习\n",
    "    :指从\" 无标注数据 \"中学习预测模型的机器学习问题.<p></p>\n",
    "&emsp;&emsp;实质:学习数据中的统计规律或潜在结构<p></p>\n",
    "强化学习\n",
    "    :指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题.<p></p>  \n",
    "&emsp;&emsp;实质:学习最优的序贯决策<p></p>\n",
    "监督和无监督的区别在于数据有无\"标注\".\n",
    "</font>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按模型分类\n",
    "### 概率与非概率\n",
    "<font size=2>\n",
    "概率模型：一定可以表示为联合概率分布的形式.&emsp;在监督学习中，属于&nbsp;生成模型<p></p>\n",
    "\n",
    "非概率模型：不一定能表示为联合概率分布的形式.&emsp;在监督学习中，属于&nbsp;判别模型<p></p>\n",
    "\n",
    "</font>\n",
    "\n",
    "### 线性与非线性\n",
    "<font size=2>\n",
    "线性模型：函数为线性函数<p></p>\n",
    "\n",
    "非线性模型：函数为非线性函数<p></p>\n",
    "\n",
    "</font>\n",
    "\n",
    "### 参数化与非参数化\n",
    "<font size=2>\n",
    "参数化模型：假设模型参数的维数固定,模型可以由有限维参数完全刻画<p></p>\n",
    "\n",
    "非参数化模型：假设模型参数的维数不固定或者无穷大,随着训练数据量的增加而不断增大<p></p>\n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按技巧分类\n",
    "<font size=2>\n",
    "贝叶斯学习  \n",
    "\n",
    "核方法\n",
    "</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见的损失函数\n",
    "<font size=3>\n",
    "（1） 0-1&nbsp;损失函数\n",
    "</font><p></p>\n",
    "\n",
    "$$ L(Y, f(X))=\\left\\{\\begin{array}{ll}\n",
    "1, & Y \\neq f(X) \\\\\n",
    "0, & Y=f(X)\n",
    "\\end{array}\\right. $$ \n",
    "\n",
    "\n",
    "<font size=3> \n",
    "（2） 平方损失函数\n",
    "</font><p></p>\n",
    "\n",
    "$$ \n",
    "L(Y, f(X))=(Y-f(X))^{2} \n",
    "$$ \n",
    "\n",
    "<font size=3> \n",
    "（3） 绝对损失函数\n",
    "</font><p></p>\n",
    "\n",
    "$$ \n",
    "L(Y, f(X))=|Y-f(X)|\n",
    "$$ \n",
    "\n",
    "<font size=3> \n",
    "（4） 对数损失函数\n",
    "</font><p></p>\n",
    "\n",
    "$$ \n",
    "L(Y, P(Y \\mid X))=-\\log P(Y \\mid X)\n",
    "$$ \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 经验风险、期望风险、结构风险\n",
    "给定一个数据集$T=\\left\\{(x_{1},y_{1}),(x_{2},y_{2}),\\cdots,(x_{n},y_{n})\\right\\}$  \n",
    "经验风险：模型$f(X)$关于训练数据集的 ***平均损失*** ,也成为 ***经验损失***，记作$R_{emp}$.\n",
    "$$R_{emp}(f)=\\frac{1}{N}\\sum_{i=1}^{N}L(y_{i},f(x_{i}))$$\n",
    "期望风险：模型关于联合分布的期望损失，记作$R_{exp}$.    \n",
    "\n",
    "**根据大数定律，当样本$N$趋近于无穷时,经验风险$R_{emp}(f)$趋于期望风险$R_{exp}(f)$**   \n",
    "\n",
    "当样本容量较小时，经验风险最小化学习的效果未必很好，易产生\"过拟合现象\",于是就有了结构风险最小化的策略。 \n",
    "\n",
    "结构风险在经验风险上加上了表示模型复杂度的***正则化***项或***罚项***，记作$R_{srm}$\n",
    "$$R_{srm}(f)=\\frac{1}{N}\\sum_{i=1}^{N}L(y_{i},f(x_{i}))+\\lambda J(f)$$  \n",
    "\n",
    "其中$J(f)$维模型的复杂度。\n",
    "\n",
    "\"结构风险最小化\"等价于\"正则化\"\n",
    "\n",
    "例子：  \n",
    "    经验风险最小化：极大似然估计    \n",
    "    结构风险最小化：贝叶斯估计  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化与交叉验证"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化\n",
    "正则化一般可以取不同的形式，例如范数\n",
    "范数 等价于 向量长度"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 交叉验证\n",
    "数据集一般可以切分为三部分：训练集、验证集、测试集。    \n",
    "训练集：用来训练模型。  \n",
    "验证集：用来选择模型。  \n",
    "测试集：用来对最终学习到的模型进行评估。    \n",
    "\n",
    "1. 简单交叉验证   \n",
    "   将数据分为两部分，一部分作为训练集，一部分作为测试集。（例如：70%的训练集，30%的测试集）\n",
    "2. S折交叉验证（应用最多）    \n",
    "   随机将数据分成S个互不相交、大小相同的子集，然后利用S-1个子集的数据训练模型，利用余下的子集测试模型，重复进行S次，选择效果最好的模型。\n",
    "3. 留一交叉验证   \n",
    "   S折交叉验证的特殊情况$S=N$，往往在数据缺乏的情况下使用。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 泛化能力"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概念\n",
    "泛化能力：指模型对未知数据的预测能力。  \n",
    "泛化误差：如果学到的模型为$\\hat{f}$,那么用这个模型对未知数据预测的误差即为泛化误差。    \n",
    "$$\\begin{split} \n",
    "R_{exp}(\\hat{f})&=E_{P}[L(Y,\\hat{f}(X))] \\\\\n",
    "&=\\int_{{X}\\times{Y}}L(y,\\hat{f}(x))P(x,y)dxdy\n",
    "\\end{split}$$   \n",
    "\n",
    "\n",
    "泛化误差上界：泛化误差的概率上界。  \n",
    "***模型的泛化能力分析往往是通过研究泛化误差的上界进行***    \n",
    "\n",
    "\n",
    "***定理1.1*** (泛化误差上界) 对于二分类问题，当假设空间是有限个函数的集合\n",
    "$F=\\left\\{f_{1},f_{2},\\cdots,f_{d}\\right\\}$时，对任意一个函数$f \\in F$,至少以概率$1-\\delta,0<\\delta<1$,以下不等式成立:\n",
    "$$R(f)\\leq \\hat{R}(f)+\\varepsilon(d,N,\\delta) \\tag{1.32}$$  \n",
    "\n",
    "$$\\varepsilon(d,N,\\delta)=\\sqrt{\\frac{1}{2N}(\\log{d}+\\log{\\frac{1}{\\delta}})} \\tag{1.33}$$\n",
    "\n",
    "不等式（1.32）左端$R(f)$是泛化误差，右端即为泛化误差上界，第1项是训练误差，训练误差越小，泛化误差越小。第2项$\\varepsilon(d,N,\\delta)$是N的单调递减函数，当$N$趋近于无穷时,趋近于0;同时它也是$\\sqrt{\\log{d}}$阶的函数,假设空间$F$包含的函数越多,其值越大.    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成模型与判别模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 常用分类器的性能指标"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础概念\n",
    "TP: 将 正类 预测为 正类 的数量  (true in positive)  \n",
    "FN: 将 正类 预测为 负类 的数量  (false in negative)  \n",
    "FP: 将 负类 预测为 正类 的数量  (false in positive)  \n",
    "TN: 将 负类 预测为 负类 的数量  (true in negative)  \n",
    "$SUM=TP+FN+FP+TN$  \n",
    "| 预测\\真实 | 正类 | 负类 |  \n",
    "|  :-----: | :----: | :----: |\n",
    "| 正类 | TP | FP |\n",
    "| 负类 | FN | TN |\n",
    "\n",
    "准确率:$\\frac{正确分类的样本数}{总样本数}=\\frac{TP+TN}{SUM}$   \n",
    "    \n",
    "意义:能反映模型预测的准确程度  \n",
    "    \n",
    "精确率：$P=\\frac{TP}{TP+TN}$  \n",
    "    \n",
    "意义：能反映预测为true中有多少正样本      \n",
    "    \n",
    "召回率：$R=\\frac{TP}{TP+FN}$  \n",
    "    \n",
    "意义：能反映正样本中有多少被预测为true    \n",
    "    \n",
    "F1值:$\\frac{2}{F1}=\\frac{1}{P}+\\frac{1}{R}$  \n",
    "\n",
    "$F1=\\frac{2TP}{2TP+FP+FN}$\n",
    "     \n",
    "意义：精确率和召回率的调和均值  \n",
    "\n",
    "案例：  \n",
    "某池塘有1400条鲤鱼，300只虾，300只鳖。现在以捕鲤鱼为目的。撒一大网，逮着了所有的鱼虾鳖。  \n",
    "\n",
    "$正确率=\\frac{1400}{1400+300+300}$  \n",
    "\n",
    "$召回率=\\frac{1400}{1400}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f46ba26180935f2312e439b1f8f9451fdb8ef327d9e4258a696a903dcdd2be8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
