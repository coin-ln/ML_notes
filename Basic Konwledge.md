# Basic Konwledge

## 统计学习的分类

### 基本分类
监督学习
    			:指从" 标注数据 "中学习预测模型的机器学习问题.
				实质:学习输入到输出的映射的统计规律
无监督学习
   			 :指从" 无标注数据 "中学习预测模型的机器学习问题.
				实质:学习数据中的统计规律或潜在结构
强化学习
    			:指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题.
				实质:学习最优的序贯决策
监督和无监督的区别在于数据有无"标注".

### 按模型分类

#### 概率与非概率

概率模型：一定可以表示为联合概率分布的形式。在监督学习中，属于生成模型

非概率模型：不一定能表示为联合概率分布的形式.在监督学习中，属于判别模型

#### 线性与非线性

线性模型：函数为线性函数

非线性模型：函数为非线性函数

#### 参数化与非参数化

参数化模型：假设模型参数的维数固定,模型可以由有限维参数完全刻画

非参数化模型：假设模型参数的维数不固定或者无穷大,随着训练数据量的增加而不断增大

#### 按技巧分类

贝叶斯学习 

核方法

## 损失函数

常见的损失函数

（1） 0-1&nbsp;损失函数
$$
L(Y, f(X))=\left\{\begin{array}{ll}
1, & Y \neq f(X) \\
0, & Y=f(X)
\end{array}\right. 
$$


 （2） 平方损失函数
$$
L(Y, f(X))=(Y-f(X))^{2}
$$

（3） 绝对损失函数

$$
L(Y, f(X))=|Y-f(X)|
$$

（4） 对数损失函数
$$
L(Y, P(Y \mid X))=-\log P(Y \mid X)
$$

## 经验风险、期望风险、结构风险

给定一个数据集$T=\left\{(x_{1},y_{1}),(x_{2},y_{2}),\cdots,(x_{n},y_{n})\right\}$  

经验风险：模型$f(X)$关于训练数据集的 ***平均损失*** ,也成为 ***经验损失***，记作$R_{emp}$.

$$R_{emp}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_{i},f(x_{i}))$$

期望风险：模型关于联合分布的期望损失，记作$R_{exp}$.   

**根据大数定律，当样$N$趋近于无穷时,经验风险$R_{emp}(f)$趋于期望风险$R_{exp}(f)$**

当样本容量较小时，经验风险最小化学习的效果未必很好，易产生"过拟合现象",于是就有了结构风险最小化的策略。 

结构风险在经验风险上加上了表示模型复杂度的**正则化项**或**罚项**，记作$R_{srm}$

$$R_{srm}(f)=\frac{1}{N}\sum_{i=1}^{N}L(y_{i},f(x_{i}))+\lambda J(f)$$  



其中$J(f)$维模型的复杂度。



"结构风险最小化"等价于"正则化"



例子：  

  经验风险最小化：极大似然估计   

  结构风险最小化：贝叶斯估计  

## 正则化与交叉验证

### 正则化

正则化一般可以取不同的形式，例如范数

范数 等价于 向量长度

### 交叉验证

数据集一般可以切分为三部分：训练集、验证集、测试集。   

训练集：用来训练模型。  

验证集：用来选择模型。  

测试集：用来对最终学习到的模型进行评估。   



1. 简单交叉验证  

  将数据分为两部分，一部分作为训练集，一部分作为测试集。（例如：70%的训练集，30%的测试集）

2. S折交叉验证（应用最多）   

  随机将数据分成S个互不相交、大小相同的子集，然后利用S-1个子集的数据训练模型，利用余下的子集测试模型，重复进行S次，选择效果最好的模型。

3. 留一交叉验证  

  S折交叉验证的特殊情况$S=N$，往往在数据缺乏的情况下使用。

## 泛化能力

泛化能力：指模型对未知数据的预测能力。  

泛化误差：如果学到的模型为$\hat{f}$,那么用这个模型对未知数据预测的误差即为泛化误差。   
$$
\begin{split} 

R_{exp}(\hat{f})&=E_{P}[L(Y,\hat{f}(X))] \\

&=\int_{{X}\times{Y}}L(y,\hat{f}(x))P(x,y)dxdy

\end{split}
$$
泛化误差上界：泛化误差的概率上界。  



**模型的泛化能力分析往往是通过研究泛化误差的上界进行**



**定理1.1** (泛化误差上界) 对于二分类问题，当假设空间是有限个函数的集合

$F=\left\{f_{1},f_{2},\cdots,f_{d}\right\}$时，对任意一个函数$f \in F$,至少以概率$1-\delta,0<\delta<1$,以下不等式成立:
$$
R(f)\leq \hat{R}(f)+\varepsilon(d,N,\delta) \tag{1.32}
$$

$$
\varepsilon(d,N,\delta)=\sqrt{\frac{1}{2N}(\log{d}+\log{\frac{1}{\delta}})} \tag{1.33}
$$



不等式（1.32）左端$R(f)$是泛化误差，右端即为泛化误差上界，第1项是训练误差，训练误差越小，泛化误差越小。第2项$\varepsilon(d,N,\delta)$是N的单调递减函数，当$N$趋近于无穷时,趋近于0;同时它也是$\sqrt{\log{d}}$阶的函数,假设空间$F$包含的函数越多,其值越大.   

## 生成模型与判别模型

 监督学习方法又可以分为生成方法和判别方法，所学到的模型分别成为**生成模型**和**判别模型**。  

生成方法：表示了给定输入$X$和产生输出$Y$的生成关系。

判别方法：数据直接学习决策函数$f(X)$或者条件概率分布$P=(Y|X)$。

#### 生成/判别 方法的特点   

生成方法：  

1. 可以还原出联合概率分布$P=(X,Y)$,判别方法则不能。     
2. 生成方法的学习收敛速度更快，即当样本容量增加的时候，学到的模型可以更快收敛于真是模型。      
3. 当存在**隐变量**时，仍可以用生成方法学习，而判别方法则不行。     

判别方法：  

1. 直接学习的是条件概率$P=(Y|X)$或决策函数$f(X)$直接面对预测，学习的 **准确率**往往更高。       
2. 由于直接学习$P=(Y|X)$或$f(X)$,可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。

## **常用分类器的性能指标**

## 基础概念
TP: 将 正类 预测为 正类 的数量  (true in positive)  
FN: 将 正类 预测为 负类 的数量  (false in negative)  
FP: 将 负类 预测为 正类 的数量  (false in positive)  
TN: 将 负类 预测为 负类 的数量  (true in negative)  
$SUM=TP+FN+FP+TN$  

| 预测\真实 | 正类 | 负类 |  
|  :-----: | :----: | :----: |
| 正类 | TP | FP |
| 负类 | FN | TN |

准确率:$\frac{正确分类的样本数}{总样本数}=\frac{TP+TN}{SUM}$   
    
意义:能反映模型预测的准确程度  
    
精确率：$P=\frac{TP}{TP+TN}$  
    
意义：能反映预测为true中有多少正样本      
    
召回率：$R=\frac{TP}{TP+FN}$  
    
意义：能反映正样本中有多少被预测为true    
    
F1值:$\frac{2}{F1}=\frac{1}{P}+\frac{1}{R}$  

$F1=\frac{2TP}{2TP+FP+FN}$
     
意义：精确率和召回率的调和均值  

案例：  
某池塘有1400条鲤鱼，300只虾，300只鳖。现在以捕鲤鱼为目的。撒一大网，逮着了所有的鱼虾鳖。  

$正确率=\frac{1400}{1400+300+300}$  

$召回率=\frac{1400}{1400}$